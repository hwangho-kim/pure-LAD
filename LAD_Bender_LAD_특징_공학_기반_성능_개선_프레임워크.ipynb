{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hwangho-kim/pure-LAD/blob/master/LAD_Bender_LAD_%ED%8A%B9%EC%A7%95_%EA%B3%B5%ED%95%99_%EA%B8%B0%EB%B0%98_%EC%84%B1%EB%8A%A5_%EA%B0%9C%EC%84%A0_%ED%94%84%EB%A0%88%EC%9E%84%EC%9B%8C%ED%81%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 라이브러리 설치 ---\n",
        "# 이 코드를 실행하기 전에 터미널에서 다음 명령어를 실행하여 필요한 라이브러리를 설치해주세요.\n",
        "!pip install scikit-learn pandas numpy matplotlib seaborn lightgbm bayesian-optimization\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import time\n",
        "import warnings\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import lightgbm as lgb\n",
        "from sklearn.datasets import load_breast_cancer, load_digits, load_wine, fetch_california_housing, make_moons\n",
        "from bayes_opt import BayesianOptimization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "# 불필요한 경고 메시지 무시\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 한국어 폰트 설정 (그래프용)\n",
        "try:\n",
        "    import matplotlib.font_manager as fm\n",
        "    font_path = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'\n",
        "    font_prop = fm.FontProperties(fname=font_path)\n",
        "    plt.rc('font', family=font_prop.get_name())\n",
        "    plt.rcParams['axes.unicode_minus'] = False\n",
        "except FileNotFoundError:\n",
        "    print(\"나눔고딕 폰트가 설치되어 있지 않아, 그래프의 한글이 깨질 수 있습니다.\")\n",
        "\n",
        "\n",
        "class LADBender:\n",
        "    \"\"\"\n",
        "    LAD-Bender: LAD를 특징 공학 도구로 사용하여 생성된 패턴 위에서\n",
        "                강력한 분류기를 훈련시키는 새로운 프레임워크\n",
        "    \"\"\"\n",
        "    def __init__(self, purity_threshold=0.9, top_features_ratio=0.5,\n",
        "                 lgbm_params=None):\n",
        "        self.purity_threshold = purity_threshold\n",
        "        self.top_features_ratio = top_features_ratio\n",
        "\n",
        "        if lgbm_params is None:\n",
        "            self.lgbm_params = {'objective': 'binary', 'verbose': -1}\n",
        "        else:\n",
        "            self.lgbm_params = lgbm_params\n",
        "\n",
        "        self.literals = []\n",
        "        self.selected_b_feature_names_ = None\n",
        "        self.candidate_patterns = []\n",
        "        # 최종 예측을 위한 분류 모델\n",
        "        self.final_classifier = LogisticRegression(random_state=42, max_iter=1000)\n",
        "\n",
        "    def _binarize(self, X: pd.DataFrame, y: pd.Series):\n",
        "        X_b = pd.DataFrame(index=X.index)\n",
        "        if not self.literals:\n",
        "            for col in X.columns:\n",
        "                if X[col].isnull().any(): continue\n",
        "                stump = DecisionTreeClassifier(max_depth=1, criterion='entropy')\n",
        "                stump.fit(X[[col]], y)\n",
        "                if stump.tree_.node_count > 1:\n",
        "                    threshold = stump.tree_.threshold[0]\n",
        "                    le_name = f\"{col}_le_{threshold:.2f}\"\n",
        "                    gt_name = f\"{col}_gt_{threshold:.2f}\"\n",
        "                    self.literals.append({'name': le_name, 'feature': col, 'op': '<=', 'val': threshold})\n",
        "                    self.literals.append({'name': gt_name, 'feature': col, 'op': '>', 'val': threshold})\n",
        "\n",
        "        for literal_info in self.literals:\n",
        "            feature, op, val, name = literal_info['feature'], literal_info['op'], literal_info['val'], literal_info['name']\n",
        "            if feature in X.columns:\n",
        "                if op == '<=': X_b[name] = (X[feature] <= val).astype(int)\n",
        "                else: X_b[name] = (X[feature] > val).astype(int)\n",
        "        return X_b\n",
        "\n",
        "    def _select_features(self, X_b: pd.DataFrame, y: pd.Series):\n",
        "        # [MODIFICATION] n_estimators를 하이퍼파라미터로 사용\n",
        "        rf = RandomForestClassifier(n_estimators=self.lgbm_params.get('n_estimators', 100), random_state=42, n_jobs=-1)\n",
        "        rf.fit(X_b, y)\n",
        "        importances = rf.feature_importances_\n",
        "        n_top_features = int(len(importances) * self.top_features_ratio)\n",
        "        if n_top_features == 0 and len(importances) > 0: n_top_features = 1\n",
        "        selected_indices = np.argsort(importances)[::-1][:n_top_features]\n",
        "        self.selected_b_feature_names_ = X_b.columns[selected_indices]\n",
        "        return X_b[self.selected_b_feature_names_]\n",
        "\n",
        "    def _generate_candidate_patterns(self, X_b_min: pd.DataFrame, y: pd.Series):\n",
        "        patterns = []\n",
        "        for target_class in [1, 0]:\n",
        "            lgbm = lgb.LGBMClassifier(**self.lgbm_params)\n",
        "            lgbm.fit(X_b_min, (y == target_class))\n",
        "            trees_df = lgbm.booster_.trees_to_dataframe()\n",
        "            seeds = self._extract_seeds(trees_df, target_class)\n",
        "            for seed in seeds:\n",
        "                prime_pattern = self._refine_to_prime(X_b_min, y, seed, target_class)\n",
        "                pattern_tuple = tuple(sorted(prime_pattern.items()))\n",
        "                if prime_pattern and pattern_tuple not in {tuple(sorted(p.items())) for p in patterns}:\n",
        "                    patterns.append(prime_pattern)\n",
        "        return patterns\n",
        "\n",
        "    def _extract_seeds(self, trees_df, target_class):\n",
        "        seeds = []\n",
        "        for tree_index in trees_df['tree_index'].unique():\n",
        "            tree = trees_df[trees_df['tree_index'] == tree_index]\n",
        "            nodes = {node['node_index']: node for _, node in tree.iterrows()}\n",
        "            if not nodes: continue\n",
        "            root_index = tree.iloc[0]['node_index']\n",
        "            def find_paths_recursive(node_index, current_path_rules):\n",
        "                node = nodes.get(node_index)\n",
        "                if node is None: return\n",
        "                is_leaf = pd.isna(node.get('left_child')) and pd.isna(node.get('right_child'))\n",
        "                if is_leaf:\n",
        "                    leaf_value = node.get('leaf_value', node.get('value'))\n",
        "                    if leaf_value is None: return\n",
        "                    prediction = 1 if leaf_value > 0 else 0\n",
        "                    if prediction == target_class and current_path_rules:\n",
        "                        seeds.append(current_path_rules.copy())\n",
        "                    return\n",
        "                feature = node.get('split_feature')\n",
        "                if feature is not None:\n",
        "                    left_child_index, right_child_index = node.get('left_child'), node.get('right_child')\n",
        "                    next_path_left = current_path_rules.copy(); next_path_left[feature] = 0\n",
        "                    find_paths_recursive(left_child_index, next_path_left)\n",
        "                    next_path_right = current_path_rules.copy(); next_path_right[feature] = 1\n",
        "                    find_paths_recursive(right_child_index, next_path_right)\n",
        "            find_paths_recursive(root_index, {})\n",
        "        return seeds\n",
        "\n",
        "    def _refine_to_prime(self, X_b, y, pattern, target_class):\n",
        "        current_pattern = pattern.copy()\n",
        "        while True:\n",
        "            removed = False\n",
        "            if len(current_pattern) <= 1: break\n",
        "            for literal_col in list(current_pattern.keys()):\n",
        "                temp_pattern = current_pattern.copy(); del temp_pattern[literal_col]\n",
        "                mask = self._get_pattern_mask(X_b, temp_pattern)\n",
        "                if mask.sum() < 3: continue\n",
        "                purity = y.loc[mask].mean()\n",
        "                is_pure_enough = (purity >= self.purity_threshold) if target_class == 1 else (purity <= (1 - self.purity_threshold))\n",
        "                if is_pure_enough:\n",
        "                    current_pattern = temp_pattern\n",
        "                    removed = True\n",
        "                    break\n",
        "            if not removed: break\n",
        "        return current_pattern\n",
        "\n",
        "    def _get_pattern_mask(self, X_b, pattern_dict):\n",
        "        mask = pd.Series(True, index=X_b.index)\n",
        "        for col, val in pattern_dict.items():\n",
        "            if col in X_b.columns:\n",
        "                mask &= (X_b[col] == val)\n",
        "        return mask\n",
        "\n",
        "    def _transform_data_with_patterns(self, X_b_min: pd.DataFrame):\n",
        "        X_transformed = pd.DataFrame(index=X_b_min.index)\n",
        "        for i, pattern in enumerate(self.candidate_patterns):\n",
        "            X_transformed[f'pattern_{i}'] = self._get_pattern_mask(X_b_min, pattern).astype(int)\n",
        "        return X_transformed\n",
        "\n",
        "    def fit(self, X_train, y_train, verbose=True):\n",
        "        if verbose: print(\"\\n[LAD-Bender] 1단계: 데이터 이진화\")\n",
        "        X_train_b = self._binarize(X_train, y_train)\n",
        "\n",
        "        if verbose: print(\"\\n[LAD-Bender] 2단계: 특징 선택\")\n",
        "        X_train_b_min = self._select_features(X_train_b, y_train)\n",
        "\n",
        "        if verbose: print(\"\\n[LAD-Bender] 3단계: 후보 패턴 생성\")\n",
        "        self.candidate_patterns = self._generate_candidate_patterns(X_train_b_min, y_train)\n",
        "        if verbose: print(f\"  > 생성된 총 후보 패턴 수: {len(self.candidate_patterns)}\")\n",
        "\n",
        "        if verbose: print(\"\\n[LAD-Bender] 4단계: 패턴 기반 특징 변환 및 최종 모델 훈련\")\n",
        "        X_train_transformed = self._transform_data_with_patterns(X_train_b_min)\n",
        "        self.final_classifier.fit(X_train_transformed, y_train)\n",
        "        if verbose: print(\"  > 최종 Logistic Regression 모델 훈련 완료.\")\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        X_test_b = pd.DataFrame(index=X_test.index)\n",
        "        for literal_info in self.literals:\n",
        "            feature, op, val, name = literal_info['feature'], literal_info['op'], literal_info['val'], literal_info['name']\n",
        "            if feature in X_test.columns:\n",
        "                if op == '<=': X_test_b[name] = (X_test[feature] <= val).astype(int)\n",
        "                else: X_test_b[name] = (X_test[feature] > val).astype(int)\n",
        "\n",
        "        for name in [l['name'] for l in self.literals]:\n",
        "            if name not in X_test_b.columns: X_test_b[name] = 0\n",
        "\n",
        "        if self.selected_b_feature_names_ is None or self.selected_b_feature_names_.empty:\n",
        "            return np.zeros(len(X_test), dtype=int)\n",
        "\n",
        "        X_test_b_min = X_test_b[self.selected_b_feature_names_]\n",
        "        X_test_transformed = self._transform_data_with_patterns(X_test_b_min)\n",
        "\n",
        "        return self.final_classifier.predict(X_test_transformed)\n",
        "\n",
        "# 베이즈 최적화를 위한 설정\n",
        "X_train_opt, y_train_opt, X_val_opt, y_val_opt = [None] * 4\n",
        "\n",
        "def black_box_function(purity_threshold, top_features_ratio, n_estimators_rf, n_estimators_lgbm):\n",
        "    top_features_ratio = max(0.1, min(1.0, top_features_ratio))\n",
        "    purity_threshold = max(0.8, min(1.0, purity_threshold))\n",
        "\n",
        "    model = LADBender(\n",
        "        purity_threshold=purity_threshold,\n",
        "        top_features_ratio=top_features_ratio,\n",
        "        lgbm_params={'n_estimators': int(n_estimators_lgbm), 'objective': 'binary', 'verbose': -1}\n",
        "    )\n",
        "    model.n_estimators_rf = int(n_estimators_rf)\n",
        "\n",
        "    model.fit(X_train_opt, y_train_opt, verbose=False)\n",
        "    y_pred = model.predict(X_val_opt)\n",
        "    return accuracy_score(y_val_opt, y_pred)\n",
        "\n",
        "# 데이터셋 로더 함수\n",
        "def load_dataset(name):\n",
        "    print(f\"\\n\\n\" + \"#\"*70)\n",
        "    print(f\"# 데이터셋: {name} 처리 시작\")\n",
        "    print(\"#\"*70)\n",
        "\n",
        "    if name == 'Breast Cancer':\n",
        "        data = load_breast_cancer()\n",
        "        X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "        y = pd.Series(np.where(data.target == 0, 1, 0), name='target')\n",
        "    elif name == 'Digits (8 vs Rest)':\n",
        "        data = load_digits()\n",
        "        X = pd.DataFrame(data.data, columns=[f'pixel_{i}' for i in range(data.data.shape[1])])\n",
        "        y = pd.Series((data.target == 8).astype(int), name='target')\n",
        "    elif name == 'Wine (Class 0 vs Rest)':\n",
        "        data = load_wine()\n",
        "        X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "        y = pd.Series((data.target == 0).astype(int), name='target')\n",
        "    elif name == 'California Housing (High/Low)':\n",
        "        data = fetch_california_housing()\n",
        "        X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "        y = pd.Series(data.target, name='target')\n",
        "        median_price = y.median()\n",
        "        y = (y > median_price).astype(int)\n",
        "    elif name == 'Moons':\n",
        "        X, y = make_moons(n_samples=1000, noise=0.3, random_state=42)\n",
        "        X = pd.DataFrame(X, columns=['feature_0', 'feature_1'])\n",
        "        y = pd.Series(y, name='target')\n",
        "    else:\n",
        "        raise ValueError(\"알 수 없는 데이터셋 이름입니다.\")\n",
        "    return X, y\n",
        "\n",
        "# 메인 실행\n",
        "if __name__ == '__main__':\n",
        "    datasets_to_run = [\n",
        "        'Breast Cancer',\n",
        "        'Digits (8 vs Rest)',\n",
        "        'Wine (Class 0 vs Rest)',\n",
        "        'California Housing (High/Low)',\n",
        "        'Moons'\n",
        "    ]\n",
        "    all_results = {}\n",
        "\n",
        "    for ds_name in datasets_to_run:\n",
        "        X, y = load_dataset(ds_name)\n",
        "\n",
        "        X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "            X, y, test_size=0.3, random_state=42, stratify=y\n",
        "        )\n",
        "        for df in [X_train_full, X_test, y_train_full, y_test]:\n",
        "            df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        X_train_opt, X_val_opt, y_train_opt, y_val_opt = train_test_split(\n",
        "            X_train_full, y_train_full, test_size=0.3, random_state=42, stratify=y_train_full\n",
        "        )\n",
        "        for df in [X_train_opt, X_val_opt, y_train_opt, y_val_opt]:\n",
        "            df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        print(\"\\n--- 베이즈 최적화를 통한 하이퍼파라미터 탐색 ---\")\n",
        "        pbounds = {\n",
        "            'purity_threshold': (0.8, 1.0),\n",
        "            'top_features_ratio': (0.1, 0.8),\n",
        "            'n_estimators_rf': (50, 200),\n",
        "            'n_estimators_lgbm': (20, 100)\n",
        "        }\n",
        "        optimizer = BayesianOptimization(f=black_box_function, pbounds=pbounds, random_state=42, verbose=2)\n",
        "        optimizer.maximize(init_points=5, n_iter=10)\n",
        "        best_params_raw = optimizer.max['params']\n",
        "\n",
        "        best_params = {\n",
        "            'purity_threshold': best_params_raw['purity_threshold'],\n",
        "            'top_features_ratio': best_params_raw['top_features_ratio'],\n",
        "            'n_estimators_rf': int(best_params_raw['n_estimators_rf']),\n",
        "            'n_estimators_lgbm': int(best_params_raw['n_estimators_lgbm']),\n",
        "            'lgbm_params': {\n",
        "                'objective': 'binary', 'verbose': -1,\n",
        "                'n_estimators': int(best_params_raw['n_estimators_lgbm'])\n",
        "            }\n",
        "        }\n",
        "\n",
        "        print(\"\\n\" + \"*\"*50)\n",
        "        print(f\"최적 파라미터:\")\n",
        "        print(f\"  - purity_threshold: {best_params['purity_threshold']:.3f}\")\n",
        "        print(f\"  - top_features_ratio: {best_params['top_features_ratio']:.3f}\")\n",
        "        print(f\"  - n_estimators_rf: {best_params['n_estimators_rf']}\")\n",
        "        print(f\"  - n_estimators_lgbm: {best_params['n_estimators_lgbm']}\")\n",
        "        print(\"*\"*50)\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        print(\"\\n--- 최적 파라미터로 LAD-Bender 훈련 ---\")\n",
        "        lad_bender_model = LADBender(\n",
        "            purity_threshold=best_params['purity_threshold'],\n",
        "            top_features_ratio=best_params['top_features_ratio'],\n",
        "            lgbm_params=best_params['lgbm_params']\n",
        "        )\n",
        "        lad_bender_model.n_estimators_rf = best_params['n_estimators_rf']\n",
        "\n",
        "        lad_bender_model.fit(X_train_full, y_train_full)\n",
        "        y_pred_lad = lad_bender_model.predict(X_test)\n",
        "        lad_acc = accuracy_score(y_test, y_pred_lad)\n",
        "\n",
        "        end_time = time.time()\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train_full)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "        # [NameError FIX] X_train, y_train -> X_train_full, y_train_full\n",
        "        models = {\n",
        "            \"Decision Tree\": DecisionTreeClassifier(random_state=42).fit(X_train_full, y_train_full).predict(X_test),\n",
        "            \"SVM\": SVC(random_state=42).fit(X_train_scaled, y_train_full).predict(X_test_scaled),\n",
        "            \"Logistic Regression\": LogisticRegression(random_state=42).fit(X_train_scaled, y_train_full).predict(X_test_scaled)\n",
        "        }\n",
        "\n",
        "        comparison_results = {\"LAD-Bender\": lad_acc}\n",
        "        for name, y_pred in models.items():\n",
        "            comparison_results[name] = accuracy_score(y_test, y_pred)\n",
        "\n",
        "        all_results[ds_name] = comparison_results\n",
        "\n",
        "        print(\"\\n\\n\" + \"#\"*70)\n",
        "        print(f\"# {ds_name} 데이터셋에 대한 최종 성능 비교 요약\")\n",
        "        print(\"#\"*70)\n",
        "\n",
        "        results_df = pd.DataFrame.from_dict(comparison_results, orient='index', columns=['Accuracy'])\n",
        "        print(results_df.to_string(float_format=\"%.4f\"))\n",
        "\n",
        "    # 전체 결과 요약 테이블 출력\n",
        "    print(\"\\n\\n\" + \"#\"*70)\n",
        "    print(\"# 모든 데이터셋에 대한 최종 성능 비교 요약\")\n",
        "    print(\"#\"*70)\n",
        "\n",
        "    summary_df = pd.DataFrame(all_results).T\n",
        "    print(summary_df.to_string(float_format=\"%.4f\"))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\n",
            "Collecting bayesian-optimization\n",
            "  Downloading bayesian_optimization-3.0.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Collecting colorama<1.0.0,>=0.4.6 (from bayesian-optimization)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading bayesian_optimization-3.0.0-py3-none-any.whl (36 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: colorama, bayesian-optimization\n",
            "Successfully installed bayesian-optimization-3.0.0 colorama-0.4.6\n",
            "나눔고딕 폰트가 설치되어 있지 않아, 그래프의 한글이 깨질 수 있습니다.\n",
            "\n",
            "\n",
            "######################################################################\n",
            "# 데이터셋: Breast Cancer 처리 시작\n",
            "######################################################################\n",
            "\n",
            "--- 베이즈 최적화를 통한 하이퍼파라미터 탐색 ---\n",
            "|   iter    |  target   | purity... | top_fe... | n_esti... | n_esti... |\n",
            "-------------------------------------------------------------------------\n",
            "| \u001b[39m2        \u001b[39m | \u001b[39m0.625    \u001b[39m | \u001b[39m0.8749080\u001b[39m | \u001b[39m0.7655000\u001b[39m | \u001b[39m159.79909\u001b[39m | \u001b[39m67.892678\u001b[39m |\n",
            "| \u001b[39m3        \u001b[39m | \u001b[39m0.625    \u001b[39m | \u001b[39m0.8312037\u001b[39m | \u001b[39m0.2091961\u001b[39m | \u001b[39m58.712541\u001b[39m | \u001b[39m89.294091\u001b[39m |\n",
            "| \u001b[39m4        \u001b[39m | \u001b[39m0.625    \u001b[39m | \u001b[39m0.9202230\u001b[39m | \u001b[39m0.5956508\u001b[39m | \u001b[39m53.087674\u001b[39m | \u001b[39m97.592788\u001b[39m |\n",
            "| \u001b[39m5        \u001b[39m | \u001b[39m0.625    \u001b[39m | \u001b[39m0.9664885\u001b[39m | \u001b[39m0.2486373\u001b[39m | \u001b[39m77.273745\u001b[39m | \u001b[39m34.672360\u001b[39m |\n",
            "| \u001b[39m6        \u001b[39m | \u001b[39m0.625    \u001b[39m | \u001b[39m0.8608484\u001b[39m | \u001b[39m0.4673295\u001b[39m | \u001b[39m114.79175\u001b[39m | \u001b[39m43.298331\u001b[39m |\n",
            "| \u001b[39m7        \u001b[39m | \u001b[39m0.625    \u001b[39m | \u001b[39m0.8766962\u001b[39m | \u001b[39m0.5629687\u001b[39m | \u001b[39m199.92635\u001b[39m | \u001b[39m99.014460\u001b[39m |\n",
            "| \u001b[39m8        \u001b[39m | \u001b[39m0.625    \u001b[39m | \u001b[39m0.9006637\u001b[39m | \u001b[39m0.6947455\u001b[39m | \u001b[39m199.67079\u001b[39m | \u001b[39m21.259519\u001b[39m |\n",
            "| \u001b[39m9        \u001b[39m | \u001b[39m0.625    \u001b[39m | \u001b[39m0.8420991\u001b[39m | \u001b[39m0.1544671\u001b[39m | \u001b[39m51.222352\u001b[39m | \u001b[39m20.120544\u001b[39m |\n",
            "| \u001b[39m10       \u001b[39m | \u001b[39m0.625    \u001b[39m | \u001b[39m0.9668150\u001b[39m | \u001b[39m0.2851669\u001b[39m | \u001b[39m199.58244\u001b[39m | \u001b[39m99.832940\u001b[39m |\n",
            "| \u001b[39m11       \u001b[39m | \u001b[39m0.625    \u001b[39m | \u001b[39m0.8292154\u001b[39m | \u001b[39m0.7925204\u001b[39m | \u001b[39m199.84718\u001b[39m | \u001b[39m20.039727\u001b[39m |\n",
            "| \u001b[39m12       \u001b[39m | \u001b[39m0.625    \u001b[39m | \u001b[39m0.9222549\u001b[39m | \u001b[39m0.4750392\u001b[39m | \u001b[39m50.345957\u001b[39m | \u001b[39m99.638167\u001b[39m |\n",
            "| \u001b[39m13       \u001b[39m | \u001b[39m0.625    \u001b[39m | \u001b[39m0.9626777\u001b[39m | \u001b[39m0.2890696\u001b[39m | \u001b[39m50.588180\u001b[39m | \u001b[39m20.045234\u001b[39m |\n",
            "| \u001b[39m14       \u001b[39m | \u001b[39m0.625    \u001b[39m | \u001b[39m0.8194130\u001b[39m | \u001b[39m0.1903697\u001b[39m | \u001b[39m199.88565\u001b[39m | \u001b[39m99.094892\u001b[39m |\n",
            "| \u001b[39m15       \u001b[39m | \u001b[39m0.625    \u001b[39m | \u001b[39m0.9423565\u001b[39m | \u001b[39m0.1519709\u001b[39m | \u001b[39m196.54409\u001b[39m | \u001b[39m20.176171\u001b[39m |\n",
            "| \u001b[39m16       \u001b[39m | \u001b[39m0.625    \u001b[39m | \u001b[39m0.9919161\u001b[39m | \u001b[39m0.5399004\u001b[39m | \u001b[39m50.496493\u001b[39m | \u001b[39m98.729461\u001b[39m |\n",
            "=========================================================================\n",
            "\n",
            "**************************************************\n",
            "최적 파라미터:\n",
            "  - purity_threshold: 0.875\n",
            "  - top_features_ratio: 0.766\n",
            "  - n_estimators_rf: 159\n",
            "  - n_estimators_lgbm: 67\n",
            "**************************************************\n",
            "\n",
            "--- 최적 파라미터로 LAD-Bender 훈련 ---\n",
            "\n",
            "[LAD-Bender] 1단계: 데이터 이진화\n",
            "\n",
            "[LAD-Bender] 2단계: 특징 선택\n",
            "\n",
            "[LAD-Bender] 3단계: 후보 패턴 생성\n",
            "  > 생성된 총 후보 패턴 수: 323\n",
            "\n",
            "[LAD-Bender] 4단계: 패턴 기반 특징 변환 및 최종 모델 훈련\n",
            "  > 최종 Logistic Regression 모델 훈련 완료.\n",
            "\n",
            "\n",
            "######################################################################\n",
            "# Breast Cancer 데이터셋에 대한 최종 성능 비교 요약\n",
            "######################################################################\n",
            "                     Accuracy\n",
            "LAD-Bender             0.6257\n",
            "Decision Tree          0.9006\n",
            "SVM                    0.9591\n",
            "Logistic Regression    0.9708\n",
            "\n",
            "\n",
            "######################################################################\n",
            "# 데이터셋: Digits (8 vs Rest) 처리 시작\n",
            "######################################################################\n",
            "\n",
            "--- 베이즈 최적화를 통한 하이퍼파라미터 탐색 ---\n",
            "|   iter    |  target   | purity... | top_fe... | n_esti... | n_esti... |\n",
            "-------------------------------------------------------------------------\n",
            "| \u001b[39m2        \u001b[39m | \u001b[39m0.9920634\u001b[39m | \u001b[39m0.8749080\u001b[39m | \u001b[39m0.7655000\u001b[39m | \u001b[39m159.79909\u001b[39m | \u001b[39m67.892678\u001b[39m |\n",
            "| \u001b[39m3        \u001b[39m | \u001b[39m0.9603174\u001b[39m | \u001b[39m0.8312037\u001b[39m | \u001b[39m0.2091961\u001b[39m | \u001b[39m58.712541\u001b[39m | \u001b[39m89.294091\u001b[39m |\n",
            "| \u001b[39m4        \u001b[39m | \u001b[39m0.9788359\u001b[39m | \u001b[39m0.9202230\u001b[39m | \u001b[39m0.5956508\u001b[39m | \u001b[39m53.087674\u001b[39m | \u001b[39m97.592788\u001b[39m |\n",
            "| \u001b[39m5        \u001b[39m | \u001b[39m0.9708994\u001b[39m | \u001b[39m0.9664885\u001b[39m | \u001b[39m0.2486373\u001b[39m | \u001b[39m77.273745\u001b[39m | \u001b[39m34.672360\u001b[39m |\n",
            "| \u001b[39m6        \u001b[39m | \u001b[39m0.9841269\u001b[39m | \u001b[39m0.8608484\u001b[39m | \u001b[39m0.4673295\u001b[39m | \u001b[39m114.79175\u001b[39m | \u001b[39m43.298331\u001b[39m |\n",
            "| \u001b[39m7        \u001b[39m | \u001b[39m0.9629629\u001b[39m | \u001b[39m0.9570351\u001b[39m | \u001b[39m0.2065035\u001b[39m | \u001b[39m107.81362\u001b[39m | \u001b[39m58.044699\u001b[39m |\n",
            "| \u001b[39m8        \u001b[39m | \u001b[39m0.9894179\u001b[39m | \u001b[39m0.8901447\u001b[39m | \u001b[39m0.5542071\u001b[39m | \u001b[39m73.045643\u001b[39m | \u001b[39m80.743962\u001b[39m |\n",
            "| \u001b[39m9        \u001b[39m | \u001b[39m0.9894179\u001b[39m | \u001b[39m0.8766918\u001b[39m | \u001b[39m0.6747792\u001b[39m | \u001b[39m160.10199\u001b[39m | \u001b[39m68.388850\u001b[39m |\n",
            "| \u001b[39m10       \u001b[39m | \u001b[39m0.9788359\u001b[39m | \u001b[39m0.9462218\u001b[39m | \u001b[39m0.3392601\u001b[39m | \u001b[39m157.01155\u001b[39m | \u001b[39m64.988390\u001b[39m |\n",
            "| \u001b[39m11       \u001b[39m | \u001b[39m0.9920634\u001b[39m | \u001b[39m0.8      \u001b[39m | \u001b[39m0.8      \u001b[39m | \u001b[39m161.57677\u001b[39m | \u001b[39m65.637317\u001b[39m |\n",
            "| \u001b[39m12       \u001b[39m | \u001b[39m0.9920634\u001b[39m | \u001b[39m0.8117493\u001b[39m | \u001b[39m0.6933042\u001b[39m | \u001b[39m77.099413\u001b[39m | \u001b[39m82.135946\u001b[39m |\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5ky3ck3W9FS",
        "outputId": "d54f2584-53c0-454c-99d4-a56988f07d5f"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}