{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hwangho-kim/pure-LAD/blob/master/EG_LAD_v4_SECOM_%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%85%8B_%EB%B0%8F_%EB%B2%A0%EC%9D%B4%EC%A6%88_%EC%B5%9C%EC%A0%81%ED%99%94_%EA%B8%B0%EB%B0%98_%EA%B2%80%EC%A6%9D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 라이브러리 설치 ---\n",
        "# 이 코드를 실행하기 전에 터미널에서 다음 명령어를 실행하여 필요한 라이브러리를 설치해주세요.\n",
        "!pip install scikit-learn pandas numpy matplotlib seaborn lightgbm bayesian-optimization\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import time\n",
        "import warnings\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_auc_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import lightgbm as lgb\n",
        "from bayes_opt import BayesianOptimization\n",
        "\n",
        "# 불필요한 경고 메시지 무시\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 한국어 폰트 설정 (그래프용)\n",
        "try:\n",
        "    import matplotlib.font_manager as fm\n",
        "    font_path = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'\n",
        "    font_prop = fm.FontProperties(fname=font_path)\n",
        "    plt.rc('font', family=font_prop.get_name())\n",
        "    plt.rcParams['axes.unicode_minus'] = False\n",
        "except FileNotFoundError:\n",
        "    print(\"나눔고딕 폰트가 설치되어 있지 않아, 그래프의 한글이 깨질 수 있습니다.\")\n",
        "    print(\"설치 방법: sudo apt-get update -qq && sudo apt-get install fonts-nanum* -qq\")\n",
        "\n",
        "\n",
        "class EnsembleGuidedLAD_v4:\n",
        "    \"\"\"\n",
        "    EG-LAD v4: '이론 형성' 단계를 커버리지 기반의 가중치 부여 방식으로 변경하여 성능 개선을 목표로 하는 프레임워크\n",
        "    \"\"\"\n",
        "    def __init__(self, purity_threshold=0.9, top_features_ratio=0.5):\n",
        "        self.purity_threshold = purity_threshold\n",
        "        self.top_features_ratio = top_features_ratio\n",
        "        self.literals = []\n",
        "        self.selected_b_feature_names_ = None\n",
        "        self.final_model_patterns = []\n",
        "\n",
        "    # --- 1단계: 최적 이진화 ---\n",
        "    def phase1_optimal_binarization(self, X: pd.DataFrame, y: pd.Series):\n",
        "        X_b = pd.DataFrame(index=X.index)\n",
        "        literal_counter = 0\n",
        "\n",
        "        for col in X.columns:\n",
        "            if X[col].isnull().any(): continue\n",
        "            stump = DecisionTreeClassifier(max_depth=1, criterion='entropy')\n",
        "            stump.fit(X[[col]], y)\n",
        "\n",
        "            if stump.tree_.node_count > 1:\n",
        "                threshold = stump.tree_.threshold[0]\n",
        "\n",
        "                literal_name_le = f\"l_{literal_counter}\"\n",
        "                self.literals.append({'name': literal_name_le, 'feature': col, 'op': '<=', 'val': threshold})\n",
        "                X_b[literal_name_le] = (X[col] <= threshold).astype(int)\n",
        "                literal_counter += 1\n",
        "\n",
        "                literal_name_gt = f\"l_{literal_counter}\"\n",
        "                self.literals.append({'name': literal_name_gt, 'feature': col, 'op': '>', 'val': threshold})\n",
        "                X_b[literal_name_gt] = (X[col] > threshold).astype(int)\n",
        "                literal_counter += 1\n",
        "        return X_b\n",
        "\n",
        "    # --- 2단계: 앙상블 기반 특징 선택 ---\n",
        "    def phase2_ensemble_feature_selection(self, X_b: pd.DataFrame, y: pd.Series):\n",
        "        rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "        rf.fit(X_b, y)\n",
        "\n",
        "        importances = rf.feature_importances_\n",
        "        n_top_features = int(len(importances) * self.top_features_ratio)\n",
        "        if n_top_features == 0 and len(importances) > 0: n_top_features = 1\n",
        "\n",
        "        selected_indices = np.argsort(importances)[::-1][:n_top_features]\n",
        "        self.selected_b_feature_names_ = X_b.columns[selected_indices]\n",
        "        return X_b[self.selected_b_feature_names_]\n",
        "\n",
        "    # --- 3단계: 계층적 패턴 생성 (HGS) ---\n",
        "    def phase3_hgs_pattern_generation(self, X_b_min: pd.DataFrame, y: pd.Series):\n",
        "        candidate_patterns = {0: [], 1: []}\n",
        "        for target_class in [1, 0]:\n",
        "            uncovered_samples_mask = (y.values == target_class)\n",
        "\n",
        "            core_patterns = self._discover_patterns_for_subset(X_b_min, y, uncovered_samples_mask, target_class)\n",
        "            candidate_patterns[target_class].extend(core_patterns)\n",
        "\n",
        "            newly_covered_mask = self._get_any_pattern_coverage(X_b_min, candidate_patterns[target_class])\n",
        "            uncovered_samples_mask = (y.values == target_class) & ~newly_covered_mask\n",
        "            if np.any(uncovered_samples_mask):\n",
        "                residual_patterns = self._discover_patterns_for_subset(X_b_min, y, uncovered_samples_mask, target_class)\n",
        "                candidate_patterns[target_class].extend(residual_patterns)\n",
        "\n",
        "            candidate_patterns[target_class] = self._prune_by_minimal_degree(candidate_patterns[target_class])\n",
        "        return candidate_patterns\n",
        "\n",
        "    def _discover_patterns_for_subset(self, X_b_all, y_all, subset_mask, target_class):\n",
        "        positive_indices = np.where(subset_mask)[0]\n",
        "        negative_indices = np.where(y_all.values != target_class)[0]\n",
        "\n",
        "        train_indices = np.concatenate([positive_indices, negative_indices])\n",
        "        if len(positive_indices) < 5 or len(negative_indices) < 5: return []\n",
        "\n",
        "        X_train = X_b_all.iloc[train_indices]\n",
        "        y_train = y_all.iloc[train_indices]\n",
        "\n",
        "        lgbm = lgb.LGBMClassifier(objective='binary', verbose=-1, n_estimators=50)\n",
        "        lgbm.fit(X_train, (y_train == target_class))\n",
        "\n",
        "        trees_df = lgbm.booster_.trees_to_dataframe()\n",
        "        seeds = self._extract_seeds(trees_df, target_class)\n",
        "\n",
        "        refined_patterns = []\n",
        "        unique_patterns = set()\n",
        "        for seed in seeds:\n",
        "            prime_pattern = self._refine_to_prime(X_b_all, y_all, seed, target_class)\n",
        "            pattern_tuple = tuple(sorted(prime_pattern.items()))\n",
        "            if prime_pattern and pattern_tuple not in unique_patterns:\n",
        "                refined_patterns.append(prime_pattern)\n",
        "                unique_patterns.add(pattern_tuple)\n",
        "        return refined_patterns\n",
        "\n",
        "    def _extract_seeds(self, trees_df, target_class):\n",
        "        seeds = []\n",
        "        for tree_index in trees_df['tree_index'].unique():\n",
        "            tree = trees_df[trees_df['tree_index'] == tree_index]\n",
        "            nodes = {node['node_index']: node for _, node in tree.iterrows()}\n",
        "            if not nodes: continue\n",
        "            root_index = tree.iloc[0]['node_index']\n",
        "            def find_paths_recursive(node_index, current_path_rules):\n",
        "                node = nodes.get(node_index)\n",
        "                if node is None: return\n",
        "                is_leaf = pd.isna(node.get('left_child')) and pd.isna(node.get('right_child'))\n",
        "                if is_leaf:\n",
        "                    leaf_value = node.get('leaf_value', node.get('value'))\n",
        "                    if leaf_value is None: return\n",
        "                    prediction = 1 if leaf_value > 0 else 0\n",
        "                    if prediction == target_class and current_path_rules:\n",
        "                        seeds.append(current_path_rules.copy())\n",
        "                    return\n",
        "                feature = node.get('split_feature')\n",
        "                if feature is not None:\n",
        "                    left_child_index, right_child_index = node.get('left_child'), node.get('right_child')\n",
        "                    next_path_left = current_path_rules.copy(); next_path_left[feature] = 0\n",
        "                    find_paths_recursive(left_child_index, next_path_left)\n",
        "                    next_path_right = current_path_rules.copy(); next_path_right[feature] = 1\n",
        "                    find_paths_recursive(right_child_index, next_path_right)\n",
        "            find_paths_recursive(root_index, {})\n",
        "        return seeds\n",
        "\n",
        "    def _refine_to_prime(self, X_b, y, pattern, target_class):\n",
        "        current_pattern = pattern.copy()\n",
        "        while True:\n",
        "            removed = False\n",
        "            if len(current_pattern) <= 1: break\n",
        "            for literal_col in list(current_pattern.keys()):\n",
        "                temp_pattern = current_pattern.copy(); del temp_pattern[literal_col]\n",
        "                mask = self._get_pattern_mask(X_b, temp_pattern)\n",
        "                if mask.sum() < 3: continue\n",
        "                purity = y.loc[mask].mean()\n",
        "                is_pure_enough = (purity >= self.purity_threshold) if target_class == 1 else (purity <= (1 - self.purity_threshold))\n",
        "                if is_pure_enough:\n",
        "                    current_pattern = temp_pattern\n",
        "                    removed = True\n",
        "                    break\n",
        "            if not removed: break\n",
        "        return current_pattern\n",
        "\n",
        "    def _prune_by_minimal_degree(self, patterns):\n",
        "        if not patterns: return []\n",
        "        literal_to_best_pattern = {}\n",
        "        for pattern in patterns:\n",
        "            degree = len(pattern)\n",
        "            for literal_name, literal_value in pattern.items():\n",
        "                key = (literal_name, literal_value)\n",
        "                if key not in literal_to_best_pattern or degree < len(literal_to_best_pattern[key]):\n",
        "                    literal_to_best_pattern[key] = pattern\n",
        "        unique_patterns = {tuple(sorted(p.items())): p for p in literal_to_best_pattern.values()}\n",
        "        return list(unique_patterns.values())\n",
        "\n",
        "    def _get_pattern_mask(self, X_b, pattern_dict):\n",
        "        mask = pd.Series(True, index=X_b.index)\n",
        "        for col, val in pattern_dict.items():\n",
        "            if col in X_b.columns:\n",
        "                mask &= (X_b[col] == val)\n",
        "        return mask\n",
        "\n",
        "    def _get_any_pattern_coverage(self, X_b, candidate_patterns):\n",
        "        final_mask = pd.Series(False, index=X_b.index)\n",
        "        for pattern in candidate_patterns:\n",
        "            final_mask |= self._get_pattern_mask(X_b, pattern)\n",
        "        return final_mask\n",
        "\n",
        "    # --- 4단계: 이론 형성 (Theory Formation) ---\n",
        "    def phase4_theory_formation(self, candidate_patterns, X_b_min, y):\n",
        "        self.final_model_patterns = []\n",
        "        for target_class, patterns in candidate_patterns.items():\n",
        "            total_class_samples = (y == target_class).sum()\n",
        "            if total_class_samples == 0: continue\n",
        "            for pattern in patterns:\n",
        "                mask = self._get_pattern_mask(X_b_min, pattern)\n",
        "                y_covered = y[mask.index[mask]]\n",
        "                covered_class_samples = y_covered[y_covered == target_class].count()\n",
        "                coverage = covered_class_samples / total_class_samples if total_class_samples > 0 else 0\n",
        "                weight = coverage if target_class == 1 else -coverage\n",
        "                self.final_model_patterns.append({'pattern': pattern, 'weight': weight, 'class': target_class})\n",
        "        self.final_model_patterns.sort(key=lambda x: abs(x['weight']), reverse=True)\n",
        "\n",
        "    def fit(self, X_train, y_train, verbose=False):\n",
        "        if verbose: print(\"\\n--- 훈련 데이터로 모델 fitting 시작 ---\")\n",
        "        X_train_b = self.phase1_optimal_binarization(X_train, y_train)\n",
        "        if verbose: print(f\"> 1단계 산출물: 이진화된 훈련 데이터 (shape: {X_train_b.shape})\")\n",
        "\n",
        "        X_train_b_min = self.phase2_ensemble_feature_selection(X_train_b, y_train)\n",
        "        if verbose: print(f\"> 2단계 산출물: 특징 선택된 훈련 데이터 (shape: {X_train_b_min.shape})\")\n",
        "\n",
        "        candidate_patterns = self.phase3_hgs_pattern_generation(X_train_b_min, y_train)\n",
        "        if verbose: print(f\"> 3단계 산출물: 후보 패턴 {sum(len(p) for p in candidate_patterns.values())}개 생성\")\n",
        "\n",
        "        self.phase4_theory_formation(candidate_patterns, X_train_b_min, y_train)\n",
        "        if verbose: print(f\"> 4단계 산출물: 최종 모델 패턴 {len(self.final_model_patterns)}개 구성\")\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        X_test_b = pd.DataFrame(0, index=X_test.index, columns=[l['name'] for l in self.literals])\n",
        "        for l in self.literals:\n",
        "            if l['feature'] in X_test.columns:\n",
        "                 if l['op'] == '<=': X_test_b.loc[X_test[l['feature']] <= l['val'], l['name']] = 1\n",
        "                 else: X_test_b.loc[X_test[l['feature']] > l['val'], l['name']] = 1\n",
        "\n",
        "        X_test_b_min = X_test_b[self.selected_b_feature_names_]\n",
        "        predictions = []\n",
        "        for i in range(len(X_test_b_min)):\n",
        "            sample = X_test_b_min.iloc[i]\n",
        "            total_score = 0.0\n",
        "            for p_info in self.final_model_patterns:\n",
        "                if all(sample.get(col) == val for col, val in p_info['pattern'].items()):\n",
        "                    total_score += p_info['weight']\n",
        "            predictions.append(1 if total_score > 0 else 0)\n",
        "        return np.array(predictions)\n",
        "\n",
        "    def print_final_patterns(self):\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"최종 모델 패턴 및 가중치 (상위 5개)\")\n",
        "        print(\"=\"*50)\n",
        "        b_name_to_literal = {l['name']: l for l in self.literals}\n",
        "        for i, p_info in enumerate(self.final_model_patterns[:5]):\n",
        "            desc = []\n",
        "            for b_feature_name, value in p_info['pattern'].items():\n",
        "                original_literal_info = b_name_to_literal.get(b_feature_name)\n",
        "                if original_literal_info:\n",
        "                    op = original_literal_info['op']\n",
        "                    if value == 0: op = '>' if op == '<=' else '<='\n",
        "                    desc.append(f\"({original_literal_info['feature']} {op} {original_literal_info['val']:.2f})\")\n",
        "            print(f\"  - 클래스 {p_info['class']} 패턴 #{i+1} (가중치: {p_info['weight']:.3f}): {' AND '.join(desc)}\")\n",
        "\n",
        "# --- 베이즈 최적화를 위한 설정 ---\n",
        "X_train_opt, y_train_opt, X_val_opt, y_val_opt = [None] * 4\n",
        "\n",
        "def black_box_function(purity_threshold, top_features_ratio):\n",
        "    \"\"\"베이즈 최적화가 평가할 목적 함수\"\"\"\n",
        "    top_features_ratio = max(0.1, min(1.0, top_features_ratio))\n",
        "    purity_threshold = max(0.7, min(1.0, purity_threshold))\n",
        "\n",
        "    model = EnsembleGuidedLAD_v4(\n",
        "        purity_threshold=purity_threshold,\n",
        "        top_features_ratio=top_features_ratio\n",
        "    )\n",
        "    model.fit(X_train_opt, y_train_opt, verbose=False)\n",
        "    y_pred = model.predict(X_val_opt)\n",
        "\n",
        "    # [MODIFICATION] 불균형 데이터셋을 위해 roc_auc_score 사용\n",
        "    # 단, 검증 세트에 두 클래스가 모두 존재할 때만 계산 가능\n",
        "    if len(np.unique(y_val_opt)) < 2:\n",
        "        return 0.5 # 한 클래스만 있으면 AUC는 0.5\n",
        "    return roc_auc_score(y_val_opt, y_pred)\n",
        "\n",
        "# --- 메인 실행 ---\n",
        "if __name__ == '__main__':\n",
        "    # 1. SECOM 데이터셋 로드 및 전처리\n",
        "    print(\"#\"*70)\n",
        "    print(\"# 데이터셋: SECOM 처리 시작\")\n",
        "    print(\"#\"*70)\n",
        "\n",
        "    try:\n",
        "        features_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom.data\"\n",
        "        labels_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom_labels.data\"\n",
        "\n",
        "        X = pd.read_csv(features_url, delim_whitespace=True, header=None)\n",
        "        X.columns = [f'feature_{i}' for i in range(X.shape[1])]\n",
        "\n",
        "        y_df = pd.read_csv(labels_url, delim_whitespace=True, header=None)\n",
        "        y = y_df[0].map({-1: 0, 1: 1})\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"SECOM 데이터셋을 로드하는 중 오류가 발생했습니다: {e}\")\n",
        "        print(\"대체 데이터셋으로 Breast Cancer를 사용합니다.\")\n",
        "        from sklearn.datasets import load_breast_cancer\n",
        "        cancer = load_breast_cancer()\n",
        "        X = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
        "        y = pd.Series(np.where(cancer.target == 0, 1, 0), name='target')\n",
        "\n",
        "    # 결측치를 각 열의 중앙값으로 대체\n",
        "    X = X.fillna(X.median())\n",
        "\n",
        "    # 2. 70/30 Train/Test 분할\n",
        "    X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "        X, y, test_size=0.3, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    # 훈련 데이터에서 분산이 0인 상수 특징 제거\n",
        "    constant_features = X_train_full.columns[X_train_full.nunique() <= 1]\n",
        "    if not constant_features.empty:\n",
        "        print(f\"  > 훈련 데이터에서 분산이 0이거나 단일 값만 갖는 특징 {len(constant_features)}개를 제거합니다.\")\n",
        "        X_train_full = X_train_full.drop(columns=constant_features)\n",
        "        X_test = X_test.drop(columns=constant_features, errors='ignore')\n",
        "\n",
        "    # 3. 베이즈 최적화를 위한 내부 분할\n",
        "    X_train_opt, X_val_opt, y_train_opt, y_val_opt = train_test_split(\n",
        "        X_train_full, y_train_full, test_size=0.3, random_state=42, stratify=y_train_full\n",
        "    )\n",
        "\n",
        "    for df in [X_train_full, X_test, y_train_full, y_test, X_train_opt, X_val_opt, y_train_opt, y_val_opt]:\n",
        "        df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    # 4. 베이즈 최적화 실행\n",
        "    print(\"\\n\" + \"#\"*70)\n",
        "    print(\"# 베이즈 최적화를 통한 하이퍼파라미터 탐색 시작\")\n",
        "    print(\"#\"*70)\n",
        "    pbounds = {'purity_threshold': (0.8, 1.0), 'top_features_ratio': (0.2, 0.8)}\n",
        "    optimizer = BayesianOptimization(f=black_box_function, pbounds=pbounds, random_state=42, verbose=2)\n",
        "    optimizer.maximize(init_points=5, n_iter=10)\n",
        "\n",
        "    best_params = optimizer.max['params']\n",
        "    print(\"\\n\" + \"*\"*50)\n",
        "    print(f\"베이즈 최적화 결과, 최적의 파라미터:\")\n",
        "    print(f\"  - purity_threshold: {best_params['purity_threshold']:.4f}\")\n",
        "    print(f\"  - top_features_ratio: {best_params['top_features_ratio']:.4f}\")\n",
        "    print(\"*\"*50)\n",
        "\n",
        "    # 5. 최적 파라미터로 최종 모델 훈련 및 평가\n",
        "    print(\"\\n\" + \"#\"*70)\n",
        "    print(\"# 최적 파라미터로 최종 모델 훈련 및 평가\")\n",
        "    print(\"#\"*70)\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    final_lad_model = EnsembleGuidedLAD_v4(\n",
        "        purity_threshold=best_params['purity_threshold'],\n",
        "        top_features_ratio=best_params['top_features_ratio']\n",
        "    )\n",
        "\n",
        "    final_lad_model.fit(X_train_full, y_train_full, verbose=True)\n",
        "    y_pred = final_lad_model.predict(X_test)\n",
        "\n",
        "    final_lad_model.print_final_patterns()\n",
        "\n",
        "    print(\"\\n[Confusion Matrix]\")\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "    print(\"\\n[Classification Report]\")\n",
        "    print(classification_report(y_test, y_pred, zero_division=0))\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"\\n최종 훈련 및 평가 시간: {end_time - start_time:.2f}초\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\n",
            "Requirement already satisfied: bayesian-optimization in /usr/local/lib/python3.11/dist-packages (3.0.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: colorama<1.0.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from bayesian-optimization) (0.4.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "나눔고딕 폰트가 설치되어 있지 않아, 그래프의 한글이 깨질 수 있습니다.\n",
            "설치 방법: sudo apt-get update -qq && sudo apt-get install fonts-nanum* -qq\n",
            "######################################################################\n",
            "# 데이터셋: SECOM 처리 시작\n",
            "######################################################################\n",
            "  > 훈련 데이터에서 분산이 0이거나 단일 값만 갖는 특징 116개를 제거합니다.\n",
            "\n",
            "######################################################################\n",
            "# 베이즈 최적화를 통한 하이퍼파라미터 탐색 시작\n",
            "######################################################################\n",
            "|   iter    |  target   | purity... | top_fe... |\n",
            "-------------------------------------------------\n",
            "| \u001b[39m2        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.8749080\u001b[39m | \u001b[39m0.7704285\u001b[39m |\n",
            "| \u001b[39m3        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.9463987\u001b[39m | \u001b[39m0.5591950\u001b[39m |\n",
            "| \u001b[39m4        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.8312037\u001b[39m | \u001b[39m0.2935967\u001b[39m |\n",
            "| \u001b[39m5        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.8116167\u001b[39m | \u001b[39m0.7197056\u001b[39m |\n",
            "| \u001b[39m6        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.9202230\u001b[39m | \u001b[39m0.6248435\u001b[39m |\n",
            "| \u001b[35m7        \u001b[39m | \u001b[35m0.5337577\u001b[39m | \u001b[35m0.9914891\u001b[39m | \u001b[35m0.2007221\u001b[39m |\n",
            "| \u001b[39m8        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.8655310\u001b[39m | \u001b[39m0.7708317\u001b[39m |\n",
            "| \u001b[39m9        \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.9030785\u001b[39m | \u001b[39m0.6721860\u001b[39m |\n",
            "| \u001b[39m10       \u001b[39m | \u001b[39m0.4983713\u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m0.2590687\u001b[39m |\n",
            "| \u001b[39m11       \u001b[39m | \u001b[39m0.5194699\u001b[39m | \u001b[39m0.9717450\u001b[39m | \u001b[39m0.2      \u001b[39m |\n",
            "| \u001b[39m12       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.8911739\u001b[39m | \u001b[39m0.6601764\u001b[39m |\n",
            "| \u001b[39m13       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.9172960\u001b[39m | \u001b[39m0.7885180\u001b[39m |\n",
            "| \u001b[35m14       \u001b[39m | \u001b[35m0.5386437\u001b[39m | \u001b[35m0.9911393\u001b[39m | \u001b[35m0.2010430\u001b[39m |\n",
            "| \u001b[39m15       \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.8800431\u001b[39m | \u001b[39m0.3220690\u001b[39m |\n",
            "| \u001b[39m16       \u001b[39m | \u001b[39m0.5337577\u001b[39m | \u001b[39m0.9914583\u001b[39m | \u001b[39m0.2007252\u001b[39m |\n",
            "=================================================\n",
            "\n",
            "**************************************************\n",
            "베이즈 최적화 결과, 최적의 파라미터:\n",
            "  - purity_threshold: 0.9911\n",
            "  - top_features_ratio: 0.2010\n",
            "**************************************************\n",
            "\n",
            "######################################################################\n",
            "# 최적 파라미터로 최종 모델 훈련 및 평가\n",
            "######################################################################\n",
            "\n",
            "--- 훈련 데이터로 모델 fitting 시작 ---\n",
            "> 1단계 산출물: 이진화된 훈련 데이터 (shape: (1096, 948))\n",
            "> 2단계 산출물: 특징 선택된 훈련 데이터 (shape: (1096, 190))\n",
            "> 3단계 산출물: 후보 패턴 380개 생성\n",
            "> 4단계 산출물: 최종 모델 패턴 380개 구성\n",
            "\n",
            "==================================================\n",
            "최종 모델 패턴 및 가중치 (상위 5개)\n",
            "==================================================\n",
            "  - 클래스 0 패턴 #1 (가중치: -0.471): (feature_25 <= 1.33) AND (feature_59 <= 8.08) AND (feature_488 > 85.04) AND (feature_299 > 0.02) AND (feature_482 <= 626.05) AND (feature_368 <= 0.00)\n",
            "  - 클래스 0 패턴 #2 (가중치: -0.293): (feature_59 <= 8.08) AND (feature_299 > 0.02) AND (feature_573 > 0.18) AND (feature_459 <= 2.67)\n",
            "  - 클래스 0 패턴 #3 (가중치: -0.273): (feature_298 <= 0.05) AND (feature_355 > 0.02) AND (feature_423 <= 97.62)\n",
            "  - 클래스 0 패턴 #4 (가중치: -0.255): (feature_100 <= 0.00) AND (feature_459 <= 2.67) AND (feature_482 <= 626.05) AND (feature_64 <= 30.74)\n",
            "  - 클래스 0 패턴 #5 (가중치: -0.227): (feature_152 <= 0.74) AND (feature_68 > 147.59) AND (feature_144 > 0.11)\n",
            "\n",
            "[Confusion Matrix]\n",
            "[[301 139]\n",
            " [ 15  16]]\n",
            "\n",
            "[Classification Report]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.68      0.80       440\n",
            "           1       0.10      0.52      0.17        31\n",
            "\n",
            "    accuracy                           0.67       471\n",
            "   macro avg       0.53      0.60      0.48       471\n",
            "weighted avg       0.90      0.67      0.76       471\n",
            "\n",
            "\n",
            "최종 훈련 및 평가 시간: 13.71초\n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXB6cXTCihgf",
        "outputId": "5ed1752c-68bf-4eb2-b48e-85959844bac5"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}